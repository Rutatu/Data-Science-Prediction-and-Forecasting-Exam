{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotly.express as px \n",
    "import plotly.graph_objects as go\n",
    "import nbformat\n",
    "from plotly.subplots import make_subplots\n",
    "import six  \n",
    "import numpy as np\n",
    "from numpy import random as rand \n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "import plotly.figure_factory as ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import RobustScaler, PowerTransformer, FunctionTransformer, KBinsDiscretizer, OrdinalEncoder, OneHotEncoder, LabelEncoder,StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline with Satisfaction Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Customer ID Referred a Friend  Number of Referrals  Tenure in Months  \\\n",
      "0  8779-QRDMV                No                    0                 1   \n",
      "1  7495-OOKFY               Yes                    1                 8   \n",
      "2  1658-BYGOY                No                    0                18   \n",
      "3  4598-XLKNJ               Yes                    1                25   \n",
      "4  4846-WHAFZ               Yes                    1                37   \n",
      "\n",
      "     Offer Phone Service  Avg Monthly Long Distance Charges Multiple Lines  \\\n",
      "0      NaN            No                               0.00             No   \n",
      "1  Offer E           Yes                              48.85            Yes   \n",
      "2  Offer D           Yes                              11.33            Yes   \n",
      "3  Offer C           Yes                              19.76             No   \n",
      "4  Offer C           Yes                               6.33            Yes   \n",
      "\n",
      "  Internet Service Internet Type  ...  Total Revenue Churn Value  \\\n",
      "0              Yes           DSL  ...          59.65           1   \n",
      "1              Yes   Fiber Optic  ...        1024.10           1   \n",
      "2              Yes   Fiber Optic  ...        1910.88           1   \n",
      "3              Yes   Fiber Optic  ...        2995.07           1   \n",
      "4              Yes   Fiber Optic  ...        3102.36           1   \n",
      "\n",
      "  Satisfaction Score  Gender Age Under 30 Senior Citizen Married Dependents  \\\n",
      "0                  3    Male  78       No            Yes      No         No   \n",
      "1                  3  Female  74       No            Yes     Yes        Yes   \n",
      "2                  2    Male  71       No            Yes      No        Yes   \n",
      "3                  2  Female  78       No            Yes     Yes        Yes   \n",
      "4                  2  Female  80       No            Yes     Yes        Yes   \n",
      "\n",
      "  Number of Dependents  \n",
      "0                    0  \n",
      "1                    1  \n",
      "2                    3  \n",
      "3                    1  \n",
      "4                    1  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df_model = pd.read_csv(\"Telco_data/df_model.csv\",  sep=\",\")\n",
    "# Display the DataFrame \n",
    "print(df_model.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Customer ID                           object\n",
       "Referred a Friend                     object\n",
       "Number of Referrals                    int64\n",
       "Tenure in Months                       int64\n",
       "Offer                                 object\n",
       "Phone Service                         object\n",
       "Avg Monthly Long Distance Charges    float64\n",
       "Multiple Lines                        object\n",
       "Internet Service                      object\n",
       "Internet Type                         object\n",
       "Avg Monthly GB Download                int64\n",
       "Online Security                       object\n",
       "Online Backup                         object\n",
       "Device Protection Plan                object\n",
       "Premium Tech Support                  object\n",
       "Streaming TV                          object\n",
       "Streaming Movies                      object\n",
       "Streaming Music                       object\n",
       "Unlimited Data                        object\n",
       "Contract                              object\n",
       "Paperless Billing                     object\n",
       "Payment Method                        object\n",
       "Monthly Charge                       float64\n",
       "Total Charges                        float64\n",
       "Total Refunds                        float64\n",
       "Total Extra Data Charges             float64\n",
       "Total Long Distance Charges          float64\n",
       "Total Revenue                        float64\n",
       "Churn Value                            int64\n",
       "Satisfaction Score                     int64\n",
       "Gender                                object\n",
       "Age                                    int64\n",
       "Under 30                              object\n",
       "Senior Citizen                        object\n",
       "Married                               object\n",
       "Dependents                            object\n",
       "Number of Dependents                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and convert object type features to categorical\n",
    "object_features = df_model.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in object_features:\n",
    "    df_model[col] = df_model[col].astype('category')\n",
    "\n",
    "# Verify the conversion\n",
    "print(df_model.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Satisfaction Score' is also treated as a categorical variable\n",
    "df_model['Satisfaction Score'] = df_model['Satisfaction Score'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var =['Age','Number of Dependents','Number of Referrals','Tenure in Months','Avg Monthly Long Distance Charges','Avg Monthly GB Download',\n",
    "              'Monthly Charge','Total Charges','Total Refunds','Total Extra Data Charges','Total Long Distance Charges','Total Revenue'] \n",
    "\n",
    "categoric_var = ['Referred a Friend', 'Phone Service', 'Multiple Lines', 'Internet Service', 'Internet Type', 'Online Security', 'Online Backup', 'Device Protection Plan', 'Premium Tech Support',\n",
    "'Streaming TV', 'Streaming Movies', 'Streaming Music', 'Unlimited Data', 'Contract', 'Paperless Billing', 'Payment Method', 'Under 30', 'Senior Citizen', 'Married', 'Dependents', 'Gender', 'Offer']\n",
    "\n",
    "\n",
    "ordinal_var = ['Satisfaction Score']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=3000, penalty='l2', C=0.1, random_state=42),  # Stronger L2 regularization\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,  # Limit the depth of the trees\n",
    "        min_samples_split=10,  # Require more samples to split nodes\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=100,\n",
    "        max_depth=6,  # Limit the depth of the trees\n",
    "        min_child_weight=5,  # Increase the minimum child weight\n",
    "        subsample=0.8,  # Use a fraction of the data for each tree\n",
    "        random_state=42\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X = df_model[numeric_var + categoric_var + ordinal_var]\n",
    "y = df_model['Churn Value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "# Encode categorical data to numeric, scale numeric data (without handling missing values) - gives same results\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categoric_var),\n",
    "        ('num', StandardScaler(), numeric_var),\n",
    "        ('ord', OrdinalEncoder(), ordinal_var)\n",
    "\n",
    "    ]) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "cv_df = []\n",
    "for name, model in models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    score = cross_validate(model_pipe, X_train, y_train, cv=10, return_train_score=False, scoring=['roc_auc', 'f1', 'recall', 'precision'])\n",
    "    mean_score = []\n",
    "    for i in score.values():\n",
    "        mean_score.append(round(np.mean(i), 3))\n",
    "    cv_df.append(pd.DataFrame({'Attribute': [i for i in score.keys()], f'{name}': mean_score }))\n",
    "\n",
    "score_before = pd.DataFrame(columns=['Attribute'])\n",
    "for i in cv_df:\n",
    "    score_before = score_before.merge(right=i, on='Attribute', how='outer')\n",
    "score_before = score_before.loc[2:].reset_index(drop=True)\n",
    "\n",
    "print(score_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting and Comparing test and train accuracies and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Fitting the models and evaluating on the test set\n",
    "for name, model in models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and performance on the training set\n",
    "    y_train_pred = model_pipe.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_error = 1 - train_accuracy  # Calculate training error\n",
    "    \n",
    "    if hasattr(model_pipe, \"predict_proba\"):\n",
    "        y_train_pred_proba = model_pipe.predict_proba(X_train)[:, 1]\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "    else:\n",
    "        train_roc_auc = None\n",
    "\n",
    "    # Predictions and performance on the test set\n",
    "    y_test_pred = model_pipe.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_error = 1 - test_accuracy  # Calculate testing error\n",
    "    \n",
    "    if hasattr(model_pipe, \"predict_proba\"):\n",
    "        y_test_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "    else:\n",
    "        test_roc_auc = None\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Train Error': train_error,  # Store training error\n",
    "        'Test Error': test_error,    # Store testing error\n",
    "        'Train ROC AUC': train_roc_auc,\n",
    "        'Test ROC AUC': test_roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"Classification report for {name} on test set:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the learning curve with a specific scoring metric\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), scoring=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(scoring if scoring else \"Score\")  # Display the scoring metric used\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for different metrics\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "for metric in metrics:\n",
    "    # Plot learning curves for each model with scoring='recall'\n",
    "    for name, model in models.items():\n",
    "        model_pipe = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        plot_learning_curve(model_pipe, f'Learning Curve for {name} ({metric})', X_train, y_train, cv=5)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cross-validation results to DataFrame\n",
    "cv_df_long = pd.melt(score_before, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Plot performance metrics comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Attribute\", y=\"Score\", hue=\"Model\", data=cv_df_long)\n",
    "plt.title('Model Performance Metrics Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over models and plot feature importance\n",
    "for name, model in models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Tree-based models\n",
    "        feature_importances = model_pipe.named_steps['model'].feature_importances_\n",
    "    elif isinstance(model, LogisticRegression):\n",
    "        # Logistic Regression\n",
    "        feature_importances = np.abs(model_pipe.named_steps['model'].coef_[0])\n",
    "    else:\n",
    "        # Skip models that don't have feature importances or coefficients\n",
    "        continue\n",
    "    \n",
    "    # Get feature names\n",
    "    cat_feature_names = model_pipe.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categoric_var)\n",
    "    ord_feature_names = ordinal_var  # Ordinal features retain their original names\n",
    "    feature_names = numeric_var + list(cat_feature_names) + ord_feature_names\n",
    "    \n",
    "    # Create DataFrame for feature importance\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title(f'Feature Importance for {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline without Satisfaction Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_var =['Age','Number of Dependents','Number of Referrals','Tenure in Months','Avg Monthly Long Distance Charges','Avg Monthly GB Download',\n",
    "              'Monthly Charge','Total Charges','Total Refunds','Total Extra Data Charges','Total Long Distance Charges','Total Revenue'] \n",
    "\n",
    "\n",
    "categoric_var = ['Referred a Friend', 'Phone Service', 'Multiple Lines', 'Internet Service', 'Internet Type', 'Online Security', 'Online Backup', 'Device Protection Plan', 'Premium Tech Support',\n",
    "'Streaming TV', 'Streaming Movies', 'Streaming Music', 'Unlimited Data', 'Contract', 'Paperless Billing', 'Payment Method', 'Under 30', 'Senior Citizen', 'Married', 'Dependents', 'Gender', 'Offer']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=3000, penalty='l2', C=0.1, random_state=42),  # Stronger L2 regularization\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,  # Limit the depth of the trees\n",
    "        min_samples_split=10,  # Require more samples to split nodes\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=100,\n",
    "        max_depth=6,  # Limit the depth of the trees\n",
    "        min_child_weight=5,  # Increase the minimum child weight\n",
    "        subsample=0.8,  # Use a fraction of the data for each tree\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# Split dataset\n",
    "X = df_model[numeric_var + categoric_var]\n",
    "y = df_model['Churn Value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "# Encode categorical data to numeric, scale numeric data (without handling missing values) - gives same results\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categoric_var),\n",
    "        ('num', StandardScaler(), numeric_var)\n",
    "        \n",
    "\n",
    "    ]) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Cross-validation\n",
    "cv_df = []\n",
    "for name, model in models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    score = cross_validate(model_pipe, X_train, y_train, cv=10, return_train_score=False, scoring=['roc_auc', 'f1', 'recall', 'precision'])\n",
    "    mean_score = []\n",
    "    for i in score.values():\n",
    "        mean_score.append(round(np.mean(i), 3))\n",
    "    cv_df.append(pd.DataFrame({'Attribute': [i for i in score.keys()], f'{name}': mean_score }))\n",
    "\n",
    "score_before = pd.DataFrame(columns=['Attribute'])\n",
    "for i in cv_df:\n",
    "    score_before = score_before.merge(right=i, on='Attribute', how='outer')\n",
    "score_before = score_before.loc[2:].reset_index(drop=True)\n",
    "\n",
    "print(score_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fit on Test set & Comparing test and train accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store results\n",
    "results = []\n",
    "\n",
    "# Fitting the models and evaluating on the test set\n",
    "for name, model in models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions and performance on the training set\n",
    "    y_train_pred = model_pipe.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    if hasattr(model_pipe, \"predict_proba\"):\n",
    "        y_train_pred_proba = model_pipe.predict_proba(X_train)[:, 1]\n",
    "        train_roc_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "    else:\n",
    "        train_roc_auc = None\n",
    "\n",
    "    # Predictions and performance on the test set\n",
    "    y_test_pred = model_pipe.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    if hasattr(model_pipe, \"predict_proba\"):\n",
    "        y_test_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "        test_roc_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
    "    else:\n",
    "        test_roc_auc = None\n",
    "\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Train Accuracy': train_accuracy,\n",
    "        'Test Accuracy': test_accuracy,\n",
    "        'Train ROC AUC': train_roc_auc,\n",
    "        'Test ROC AUC': test_roc_auc\n",
    "    })\n",
    "    \n",
    "    print(f\"Classification report for {name} on test set:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Define a function to plot the learning curve with a specific scoring metric\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), scoring=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(scoring if scoring else \"Score\")  # Display the scoring metric used\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "# Plot learning curves for different metrics\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "for metric in metrics:\n",
    "# Plot learning curves for each model with scoring='recall'\n",
    "    for name, model in models.items():\n",
    "        model_pipe = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "        ])\n",
    "        plot_learning_curve(model_pipe, f'Learning Curve for {name} ({metric})', X_train, y_train, cv=5, scoring='recall')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cross-validation results to DataFrame\n",
    "cv_df_long = pd.melt(score_before, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Plot performance metrics comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Attribute\", y=\"Score\", hue=\"Model\", data=cv_df_long)\n",
    "plt.title('Model Performance Metrics Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance for tree-based models and logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over models and plot feature importance\n",
    "for name, model in models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor), ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        # Tree-based models\n",
    "        feature_importances = model_pipe.named_steps['model'].feature_importances_\n",
    "    elif isinstance(model, LogisticRegression):\n",
    "        # Logistic Regression\n",
    "        feature_importances = np.abs(model_pipe.named_steps['model'].coef_[0])\n",
    "    else:\n",
    "        # Skip models that don't have feature importances or coefficients\n",
    "        continue\n",
    "    \n",
    "    # Get feature names\n",
    "    cat_feature_names = model_pipe.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categoric_var)\n",
    "    feature_names = numeric_var + list(cat_feature_names)\n",
    "    \n",
    "    # Create DataFrame for feature importance\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title(f'Feature Importance for {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra Data Preprocessing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.stats import skew, shapiro, normaltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the encoding switch to df_model\n",
    "#  ---> from ['Yes', 'No'] to  ['No', 'Yes']\n",
    "\n",
    "columns_to_switch = ['Internet Service', 'Device Protection Plan', 'Streaming Movies', 'Paperless Billing', 'Senior Citizen']\n",
    "\n",
    "\n",
    "for col in columns_to_switch:\n",
    "    df_model[col] = pd.Categorical(df_model[col], categories=['No', 'Yes'], ordered=True)\n",
    "\n",
    "\n",
    "# Define the categorical and numerical variables\n",
    "binary_var = ['Referred a Friend', 'Phone Service', 'Multiple Lines', 'Online Security', 'Online Backup', \n",
    "              'Device Protection Plan', 'Premium Tech Support', 'Streaming TV', 'Streaming Movies', \n",
    "              'Streaming Music', 'Unlimited Data', 'Paperless Billing', 'Under 30', 'Senior Citizen', \n",
    "              'Married', 'Dependents','Offer', 'Payment Method', 'Gender', 'Contract']\n",
    "\n",
    "\n",
    "# Verify the order and print unique values\n",
    "for col in binary_var:\n",
    "    print('Variable :', col)\n",
    "    print('Unique Values :', df_model[col].unique())\n",
    "    print('Number of Unique Values :', df_model[col].nunique())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find outliers\n",
    "def find_outliers_IQR(Series):\n",
    "   q1=Series.quantile(0.25)\n",
    "   q3=Series.quantile(0.75)\n",
    "   IQR=q3-q1 \n",
    "   upper_bound = (q3+1.5*IQR) \n",
    "   lower_bound = (q1-1.5*IQR)\n",
    "   outliers = Series[(Series<lower_bound) | (Series>upper_bound)]\n",
    "   return outliers, lower_bound, upper_bound \n",
    "\n",
    "# plot data distributions\n",
    "plt.figure(figsize=(20,10))\n",
    "for i in enumerate(numeric_var):\n",
    "  plt.subplot(5,4,i[0]+1)\n",
    "  sns.histplot(df_model[i[1]], kde=True,color='green')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_conc = pd.DataFrame({\n",
    "    'Numeric Var':numeric_var,\n",
    "    'Skewness':[ round(skew(df_model[i], bias=False),3) for i in numeric_var],\n",
    "    'Shapiro P-val': [ round(shapiro(df_model[i].sample(3000))[1],5) for i in numeric_var] ,\n",
    "    'Conclusion': [ 'Normal' if shapiro(df_model[i].sample(3000))[1] >= 0.05 else 'Non-Normal' for i in numeric_var],\n",
    "    'Outlier Count': [find_outliers_IQR(df_model[i])[0].shape[0] for i in numeric_var],\n",
    "    'Outlier %' : [ round((i / df_model.shape[0])*100,2) for i in [find_outliers_IQR(df_model[i])[0].shape[0] for i in numeric_var]]  \n",
    "}).sort_values(by='Skewness', ignore_index=True)\n",
    "display(num_conc,df_model[numeric_var].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers: 'Number of Dependents', 'Total Extra Data Charges', 'Total Refunds' , 'Number of Referrals', 'Total Long Distance Charges','Avg Monthly GB Download','Total Revenue'\n",
    "\n",
    "The following outlier operations will be carried out:\n",
    "- For the feature 'Number of Dependents' all numbers above 3 will be changed to number 3.\n",
    "- For the features 'Total Extra Data Charges' and 'Total Refunds' they will be minimized and changed to categorical data: all numbers 0 will be changed to 'No', the rest to 'Yes'.\n",
    "- For other features the outlier will be considered as missing values ​​which will later be compiled using KNN Imputer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting outlier variables\n",
    "out_var1 = ['Total Refunds','Total Extra Data Charges'] \n",
    "out_var2 = ['Number of Dependents']\n",
    "out_var3 = ['Number of Referrals','Total Long Distance Charges','Avg Monthly GB Download','Total Revenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of the values 0\n",
    "for i in ['Total Extra Data Charges','Total Refunds']: \n",
    "    print( \"{} == 0 :\".format(i),df_model[i].value_counts().head(1)[0], '| {persen} %'.format(persen=round((df_model[i].value_counts().head(1)[0]/df_model.shape[0])*100,3)))\n",
    "    print(\"{} != 0 :\".format(i), df_model[df_model[i] !=0].shape[0], '| {persen} %'.format(persen=round((df_model[df_model[i] !=0].shape[0] / df_model.shape[0]) * 100,3)))\n",
    "    print('\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform = df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode the fetures with outliers to categorical (Yes and No categories)\n",
    "df_transform['Total Extra Data Charges'] = df_transform['Total Extra Data Charges'].apply(lambda x: 'No' if x == 0 else 'Yes')\n",
    "df_transform['Total Refunds'] = df_transform['Total Refunds'].apply(lambda x: 'No' if x == 0 else 'Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chech the encoding worked\n",
    "print(df_transform['Total Extra Data Charges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the order and print unique values\n",
    "binary_variables = ['Total Extra Data Charges','Total Refunds']\n",
    "\n",
    "for col in binary_variables:\n",
    "    print('Variable :', col)\n",
    "    print('Unique Values :', df_transform[col].unique())\n",
    "    print('Number of Unique Values :', df_transform[col].nunique())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the encoding switch to df_model\n",
    "#  ---> from ['Yes', 'No'] to  ['No', 'Yes']\n",
    "\n",
    "columns_to_switch_2 = ['Total Extra Data Charges']\n",
    "\n",
    "\n",
    "for col in columns_to_switch_2:\n",
    "    df_transform[col] = pd.Categorical(df_transform[col], categories=['No', 'Yes'], ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_variables = ['Total Extra Data Charges','Total Refunds']\n",
    "\n",
    "# Verify the order and print unique values\n",
    "\n",
    "for col in binary_variables:\n",
    "    print('Variable :', col)\n",
    "    print('Unique Values :', df_transform[col].unique())\n",
    "    print('Number of Unique Values :', df_transform[col].nunique())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IQR method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caps the values of 'Number of Dependents' at 3 \n",
    "\n",
    "Identifies outliers in specified columns using the IQR method and replaces values above the upper bound with NaN. \n",
    "\n",
    "Imputes the NaN values using the KNN imputer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer, SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find outliers\n",
    "def find_outliers_IQR(Series):\n",
    "   q1=Series.quantile(0.25)\n",
    "   q3=Series.quantile(0.75)\n",
    "   IQR=q3-q1 \n",
    "   upper_bound = (q3+1.5*IQR) \n",
    "   lower_bound = (q1-1.5*IQR)\n",
    "   outliers = Series[(Series<lower_bound) | (Series>upper_bound)]\n",
    "   return outliers, lower_bound, upper_bound \n",
    "\n",
    "# function handling outliers\n",
    "def func_handling_outlier(X_train):\n",
    "    # handling variable 2 outliers\n",
    "    X_train['Number of Dependents'] = X_train['Number of Dependents'].apply(lambda i: 3 if i>3 else i) \n",
    "    # handling variable 3 outliers\n",
    "    out_var3 = ['Number of Referrals','Total Long Distance Charges','Avg Monthly GB Download','Total Revenue']\n",
    "    # upper bound\n",
    "    upper_bound = []\n",
    "    for i in out_var3:\n",
    "        upper_bound.append( int(find_outliers_IQR(df_transform[i])[2]) ) \n",
    "    #  replace values that are above the upper bound with NaN\n",
    "    for i,j in zip(out_var3,upper_bound):    \n",
    "        X_train.loc[X_train[X_train[i]>j].index.to_list(),i] = np.NaN\n",
    "    # impute the NaN values using the K-Nearest Neighbors (KNN) imputer\n",
    "    imputer = KNNImputer(n_neighbors=7) \n",
    "    X_train[out_var3] = imputer.fit_transform(X_train[out_var3])\n",
    "    X_train[out_var3] = X_train[out_var3]\n",
    "    return X_train[numeric_var + ordinal_var + nominal_var]\n",
    "\n",
    "\n",
    "# pipeline for handling outlier\n",
    "hand_outliers = ColumnTransformer([\n",
    "    ('handling_outlier', FunctionTransformer(func_handling_outlier),numeric_var + ordinal_var + nominal_var)\n",
    "    ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection without Satisfaction Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order for ordinal variables\n",
    "## excluded variables: 'Churn Value', 'Satisfaction Score', 'Customer ID', Adding 'Total Extra Data Charges' and 'Total Refunds' as categorical\n",
    "ord_ = {\n",
    "    'ord1': [['Married', 'Dependents', 'Referred a Friend', 'Phone Service', 'Premium Tech Support', 'Streaming Music', 'Unlimited Data', 'Under 30', 'Online Security',\n",
    "             'Online Backup','Streaming TV', 'Multiple Lines', 'Internet Service','Device Protection Plan', 'Streaming Movies', 'Paperless Billing', 'Senior Citizen',\n",
    "             'Total Extra Data Charges','Total Refunds'], ['No', 'Yes']],\n",
    "    'ord2': [['Internet Type'], ['None', 'DSL', 'Cable', 'Fiber Optic']], # MAKE SURE THE SPEED OF THE INTERNET TYPE INCREASING\n",
    "    'ord3': [['Contract'], ['Month-to-Month', 'One Year', 'Two Year']]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "nominal_var = ['Offer', 'Gender', 'Payment Method']\n",
    "\n",
    "\n",
    "numeric_var = ['Number of Referrals', 'Tenure in Months', 'Avg Monthly Long Distance Charges', \n",
    "               'Avg Monthly GB Download', 'Monthly Charge', 'Total Charges', 'Total Long Distance Charges', \n",
    "               'Total Revenue', 'Age', 'Number of Dependents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection models\n",
    "feature_selection_models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,  # Limit the depth of the trees\n",
    "        min_samples_split=10,  # Require more samples to split nodes\n",
    "        random_state=42\n",
    "    ),\n",
    "    'XGBoost': XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=100,\n",
    "        max_depth=6,  # Limit the depth of the trees\n",
    "        min_child_weight=5,  # Increase the minimum child weight\n",
    "        subsample=0.8,  # Use a fraction of the data for each tree\n",
    "        random_state=42\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fit, Feature Importance and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all ordinal variables into one list and their respective order categories\n",
    "ordinal_var = []\n",
    "ord_categories = []\n",
    "for key in ord_:\n",
    "    ordinal_var += ord_[key][0]\n",
    "    ord_categories += [ord_[key][1]] * len(ord_[key][0])\n",
    "\n",
    "\n",
    "# Define function to find outliers\n",
    "def find_outliers_IQR(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    IQR = q3 - q1\n",
    "    upper_bound = q3 + 1.5 * IQR\n",
    "    lower_bound = q1 - 1.5 * IQR\n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Function for handling outliers\n",
    "def func_handling_outlier(X):\n",
    "    X = X.copy()\n",
    "    X['Number of Dependents'] = X['Number of Dependents'].apply(lambda i: 3 if i > 3 else i)\n",
    "    out_var3 = ['Number of Referrals', 'Total Long Distance Charges', 'Avg Monthly GB Download', 'Total Revenue']\n",
    "    upper_bound = []\n",
    "    for i in out_var3:\n",
    "        upper_bound.append(int(find_outliers_IQR(X[i])[2]))\n",
    "    for i, j in zip(out_var3, upper_bound):\n",
    "        X.loc[X[X[i] > j].index.to_list(), i] = np.NaN\n",
    "    imputer = KNNImputer(n_neighbors=7)\n",
    "    X[out_var3] = imputer.fit_transform(X[out_var3])\n",
    "    return X\n",
    "\n",
    "# Split dataset\n",
    "X = df_transform[numeric_var + nominal_var + ordinal_var]\n",
    "y = df_transform['Churn Value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Ensure data is in DataFrame format\n",
    "X_train = pd.DataFrame(X_train, columns=numeric_var + nominal_var + ordinal_var)\n",
    "X_test = pd.DataFrame(X_test, columns=numeric_var + nominal_var + ordinal_var)\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "nominal_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "ordinal_pipeline = Pipeline(steps=[\n",
    "    ('encoder', OrdinalEncoder(categories=ord_categories))\n",
    "])\n",
    "\n",
    "# Column transformer for preprocessing\n",
    "prep_var = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numeric_var),\n",
    "        ('nominal', nominal_pipeline, nominal_var),\n",
    "        ('ordinal', ordinal_pipeline, ordinal_var)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Pipeline for handling outliers\n",
    "hand_outliers = Pipeline(steps=[\n",
    "    ('handling_outlier', FunctionTransformer(func_handling_outlier))\n",
    "])\n",
    "\n",
    "# Combined preprocessor\n",
    "combined_preprocessor = Pipeline(steps=[\n",
    "    ('hand_outliers', hand_outliers),\n",
    "    ('prep_var', prep_var)\n",
    "])\n",
    "\n",
    "# Fit the combined preprocessor first\n",
    "combined_preprocessor.fit(X_train)\n",
    "\n",
    "# Extract feature names after fitting the combined preprocessor\n",
    "nominal_feature_names = combined_preprocessor.named_steps['prep_var'].named_transformers_['nominal'].named_steps['encoder'].get_feature_names_out(nominal_var)\n",
    "feature_names = numeric_var + list(nominal_feature_names) + ordinal_var\n",
    "\n",
    "# Feature selection models\n",
    "feature_selection_models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,  # Increased number of trees\n",
    "        max_depth=10,  # No limit on the depth of the trees\n",
    "        min_samples_split=10,  # Lowered to allow more splits\n",
    "        random_state=42\n",
    ")\n",
    ",\n",
    "    'XGBoost': XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=200,  # Increased number of boosting rounds\n",
    "        max_depth=6,  # Keep the same depth\n",
    "        min_child_weight=5,  # Reduced to allow more splits\n",
    "        subsample=0.8,  # Keep the same subsample ratio\n",
    "        learning_rate=0.1,  # Introduce a learning rate\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv_df_fs = []\n",
    "for name, model in feature_selection_models.items():\n",
    "    feature_selector = SelectFromModel(model, threshold='mean')\n",
    "    \n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    score = cross_validate(model_pipe, X_train, y_train, cv=10, return_train_score=False, scoring=['roc_auc', 'f1', 'recall', 'precision'])\n",
    "    mean_score = []\n",
    "    for i in score.values():\n",
    "        mean_score.append(round(np.mean(i), 3))\n",
    "    cv_df_fs.append(pd.DataFrame({'Attribute': [i for i in score.keys()], f'{name}': mean_score}))\n",
    "\n",
    "score_fs = pd.DataFrame(columns=['Attribute'])\n",
    "for i in cv_df_fs:\n",
    "    score_fs = score_fs.merge(right=i, on='Attribute', how='outer')\n",
    "score_fs = score_fs.loc[2:].reset_index(drop=True)\n",
    "\n",
    "print(\"Feature Selection Model Cross-Validation Results:\")\n",
    "print(score_fs)\n",
    "\n",
    "\n",
    "\n",
    "model_pipes = {}\n",
    "# Fitting the feature selection models and evaluating on the test set\n",
    "for name, model in feature_selection_models.items():\n",
    "    feature_selector = SelectFromModel(model, threshold='mean')\n",
    "    \n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    model_pipes[name] = model_pipe\n",
    "    \n",
    "    # Extract selected feature names\n",
    "    selected_mask = model_pipe.named_steps['feature_selection'].get_support()\n",
    "    selected_features = [feature for feature, selected in zip(feature_names, selected_mask) if selected]\n",
    "\n",
    "    print(f\"Selected Features for {name}: {selected_features}\")\n",
    "\n",
    "    # Transform the training and testing sets using the entire pipeline\n",
    "    X_train_transformed = model_pipe.named_steps['preprocessor'].transform(X_train)\n",
    "    X_test_transformed = model_pipe.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "    # Transform using the feature selector\n",
    "    X_train_selected = model_pipe.named_steps['feature_selection'].transform(X_train_transformed)\n",
    "    X_test_selected = model_pipe.named_steps['feature_selection'].transform(X_test_transformed)\n",
    "\n",
    "    # Create a new model pipeline with only the selected features\n",
    "    final_model_pipe = Pipeline(steps=[\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Fit the final model on the selected features\n",
    "    final_model_pipe.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    y_pred = final_model_pipe.predict(X_test_selected)\n",
    "    print(f\"Classification report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plot feature importances of the selected features\n",
    "    importances = model_pipe.named_steps['feature_selection'].estimator_.feature_importances_\n",
    "    selected_importances = [importance for importance, selected in zip(importances, selected_mask) if selected]\n",
    "    importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': selected_importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title(f'Feature Importance for {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Define a function to plot the learning curve with a specific scoring metric\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), scoring=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(scoring if scoring else \"Score\")  # Display the scoring metric used\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Plot learning curves for each model with scoring='recall'\n",
    "for name, model in feature_selection_models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    plot_learning_curve(model_pipe, f'Learning Curve for {name}', X_train, y_train, cv=5, scoring='recall')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cross-validation results to DataFrame\n",
    "cv_df_long = pd.melt(score_fs, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Plot performance metrics comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Attribute\", y=\"Score\", hue=\"Model\", data=cv_df_long)\n",
    "plt.title('Model Performance Metrics Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in feature_selection_models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all ordinal variables into one list and their respective order categories\n",
    "ordinal_var = []\n",
    "ord_categories = []\n",
    "for key in ord_:\n",
    "    ordinal_var += ord_[key][0]\n",
    "    ord_categories += [ord_[key][1]] * len(ord_[key][0])\n",
    "\n",
    "# Define function to find outliers\n",
    "def find_outliers_IQR(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    IQR = q3 - q1\n",
    "    upper_bound = q3 + 1.5 * IQR\n",
    "    lower_bound = q1 - 1.5 * IQR\n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Function for handling outliers\n",
    "def func_handling_outlier(X):\n",
    "    X = X.copy()\n",
    "    X['Number of Dependents'] = X['Number of Dependents'].apply(lambda i: 3 if i > 3 else i)\n",
    "    out_var3 = ['Number of Referrals', 'Total Long Distance Charges', 'Avg Monthly GB Download', 'Total Revenue']\n",
    "    upper_bound = []\n",
    "    for i in out_var3:\n",
    "        upper_bound.append(int(find_outliers_IQR(X[i])[2]))\n",
    "    for i, j in zip(out_var3, upper_bound):\n",
    "        X.loc[X[X[i] > j].index.to_list(), i] = np.NaN\n",
    "    imputer = KNNImputer(n_neighbors=7)\n",
    "    X[out_var3] = imputer.fit_transform(X[out_var3])\n",
    "    return X\n",
    "\n",
    "# Split dataset\n",
    "X = df_transform[numeric_var + nominal_var + ordinal_var]\n",
    "y = df_transform['Churn Value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Ensure data is in DataFrame format\n",
    "X_train = pd.DataFrame(X_train, columns=numeric_var + nominal_var + ordinal_var)\n",
    "X_test = pd.DataFrame(X_test, columns=numeric_var + nominal_var + ordinal_var)\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "nominal_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "ordinal_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OrdinalEncoder(categories=ord_categories))\n",
    "])\n",
    "\n",
    "# Column transformer for preprocessing\n",
    "prep_var = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numeric_var),\n",
    "        ('nominal', nominal_pipeline, nominal_var),\n",
    "        ('ordinal', ordinal_pipeline, ordinal_var)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Pipeline for handling outliers\n",
    "hand_outliers = Pipeline(steps=[\n",
    "    ('handling_outlier', FunctionTransformer(func_handling_outlier))\n",
    "])\n",
    "\n",
    "# Combined preprocessor\n",
    "combined_preprocessor = Pipeline(steps=[\n",
    "    ('hand_outliers', hand_outliers),\n",
    "    ('prep_var', prep_var)\n",
    "])\n",
    "\n",
    "# Fit the combined preprocessor first\n",
    "combined_preprocessor.fit(X_train)\n",
    "\n",
    "# Extract feature names after fitting the combined preprocessor\n",
    "nominal_feature_names = combined_preprocessor.named_steps['prep_var'].named_transformers_['nominal'].named_steps['encoder'].get_feature_names_out(nominal_var)\n",
    "feature_names = numeric_var + list(nominal_feature_names) + ordinal_var\n",
    "\n",
    "# Feature selection with Logistic Regression using RFE\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "feature_selector = RFE(model, n_features_to_select=10)  # Select top 10 features\n",
    "\n",
    "model_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', combined_preprocessor),\n",
    "    ('feature_selection', feature_selector),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "cv_df_fs_LR = []\n",
    "\n",
    "# Perform cross-validation\n",
    "score = cross_validate(model_pipe, X_train, y_train, cv=10, return_train_score=False, scoring=['roc_auc', 'f1', 'recall', 'precision'])\n",
    "\n",
    "# Convert cross-validation results to DataFrame\n",
    "cv_df_fs_LR = pd.DataFrame({\n",
    "    'Attribute': [i for i in score.keys()],\n",
    "    'Logistic Regression': [round(np.mean(score[i]), 3) for i in score.keys()]\n",
    "})\n",
    "\n",
    "print(\"Feature Selection Model Cross-Validation Results:\")\n",
    "print(cv_df_fs_LR)\n",
    "\n",
    "# Directly assign to score_fs_LR\n",
    "score_fs_LR = cv_df_fs_LR.loc[2:].reset_index(drop=True)\n",
    "\n",
    "# Fitting the feature selection model and evaluating on the test set\n",
    "model_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Extract selected feature names\n",
    "selected_mask = model_pipe.named_steps['feature_selection'].get_support()\n",
    "selected_features = [feature for feature, selected in zip(feature_names, selected_mask) if selected]\n",
    "\n",
    "print(f\"Selected Features for Logistic Regression: {selected_features}\")\n",
    "\n",
    "# Transform the training and testing sets using the entire pipeline\n",
    "X_train_transformed = model_pipe.named_steps['preprocessor'].transform(X_train)\n",
    "X_test_transformed = model_pipe.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Transform using the feature selector\n",
    "X_train_selected = model_pipe.named_steps['feature_selection'].transform(X_train_transformed)\n",
    "X_test_selected = model_pipe.named_steps['feature_selection'].transform(X_test_transformed)\n",
    "\n",
    "# Create a new model pipeline with only the selected features\n",
    "final_model_pipe = Pipeline(steps=[\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Fit the final model on the selected features\n",
    "final_model_pipe.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "y_pred = final_model_pipe.predict(X_test_selected)\n",
    "print(f\"Classification report for Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Plot learning curves for different metrics\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "for metric in metrics:\n",
    "    plot_learning_curve(model_pipe, f'Learning Curve for Logistic Regression ({metric})', X_train, y_train, cv=5, scoring=metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "coefficients = final_model_pipe.named_steps['model'].coef_[0]\n",
    "importance_df = pd.DataFrame({'Feature': selected_features, 'Coefficient': coefficients})\n",
    "importance_df = importance_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importance for Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cross-validation results to long format\n",
    "cv_df_long_LR = pd.melt(cv_df_fs_LR, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Define the desired order of metrics\n",
    "desired_order = ['test_f1', 'test_precision', 'test_recall', 'test_roc_auc']\n",
    "\n",
    "# Convert the 'Attribute' column to a categorical type with the specified order\n",
    "cv_df_long_LR['Attribute'] = pd.Categorical(cv_df_long_LR['Attribute'], categories=desired_order, ordered=True)\n",
    "\n",
    "# Plot performance metrics comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Attribute\", y=\"Score\", hue=\"Model\", data=cv_df_long_LR)\n",
    "plt.title('Logistic Regression Performance Metrics Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Logistic Regression cross-validation results to long format\n",
    "cv_df_long_LR = pd.melt(cv_df_fs_LR, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Convert other models' cross-validation results to long format\n",
    "cv_df_long_other = pd.melt(score_fs, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Ensure there are no duplicate entries\n",
    "cv_df_long_LR = cv_df_long_LR.drop_duplicates(subset=['Attribute', 'Model'])\n",
    "cv_df_long_other = cv_df_long_other.drop_duplicates(subset=['Attribute', 'Model'])\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_df_long = pd.concat([cv_df_long_LR, cv_df_long_other], axis=0)\n",
    "\n",
    "# Reset index to ensure no duplicate indices\n",
    "combined_df_long = combined_df_long.reset_index(drop=True)\n",
    "\n",
    "# Define the desired order of metrics\n",
    "desired_order = ['test_f1', 'test_precision', 'test_recall', 'test_roc_auc']\n",
    "\n",
    "# Convert the 'Attribute' column to a categorical type with the specified order\n",
    "combined_df_long['Attribute'] = pd.Categorical(combined_df_long['Attribute'], categories=desired_order, ordered=True)\n",
    "\n",
    "# Plot performance metrics comparison for all models\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Attribute\", y=\"Score\", hue=\"Model\", data=combined_df_long)\n",
    "plt.title('Model Performance Metrics Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve for Logistic Regression\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define the Logistic Regression pipeline\n",
    "model_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', combined_preprocessor),\n",
    "    ('feature_selection', feature_selector),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "model_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, lw=2, label=f'Logistic Regression (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the diagonal line representing random guessing\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined ROC AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot ROC curves for other models in feature_selection_models\n",
    "for name, model in feature_selection_models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot ROC curve for Logistic Regression\n",
    "# Define the Logistic Regression pipeline\n",
    "model_pipe_lr = Pipeline(steps=[\n",
    "    ('preprocessor', combined_preprocessor),\n",
    "    ('feature_selection', feature_selector),\n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "model_pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_lr = model_pipe_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC for Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "# Plot the ROC curve for Logistic Regression\n",
    "plt.plot(fpr_lr, tpr_lr, lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})', color='red')  # Use a distinct color\n",
    "\n",
    "# Plot the diagonal line representing random guessing\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection with Satisfaction Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fit, Feature Importance and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the order for ordinal variables\n",
    "## excluded variables: 'Churn Value', 'Customer ID'\n",
    "ord_ = {\n",
    "    'ord1': [['Married', 'Dependents', 'Referred a Friend', 'Phone Service', 'Premium Tech Support', 'Streaming Music', 'Unlimited Data', 'Under 30', 'Online Security',\n",
    "             'Online Backup','Streaming TV', 'Multiple Lines', 'Internet Service','Device Protection Plan', 'Streaming Movies', 'Paperless Billing', 'Senior Citizen', \n",
    "             'Total Extra Data Charges','Total Refunds'], ['No', 'Yes']],\n",
    "    'ord2': [['Internet Type'], ['None', 'DSL', 'Cable', 'Fiber Optic']], # MAKE SURE THE SPEED OF THE INTERNET TYPE INCREASING\n",
    "    'ord3': [['Contract'], ['Month-to-Month', 'One Year', 'Two Year']],\n",
    "    'ord4': [['Satisfaction Score'], ['1', '2', '3', '4', '5']]\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "nominal_var = ['Offer', 'Gender', 'Payment Method']\n",
    "\n",
    "\n",
    "numeric_var = ['Number of Referrals', 'Tenure in Months', 'Avg Monthly Long Distance Charges', \n",
    "               'Avg Monthly GB Download', 'Monthly Charge', 'Total Charges', 'Total Long Distance Charges', 'Total Revenue', 'Age', 'Number of Dependents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'Satisfaction Score' is also treated as a categorical variable\n",
    "df_transform['Satisfaction Score'] = df_transform['Satisfaction Score'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all ordinal variables into one list and their respective order categories\n",
    "ordinal_var = []\n",
    "ord_categories = []\n",
    "for key in ord_:\n",
    "    ordinal_var += ord_[key][0]\n",
    "    ord_categories += [ord_[key][1]] * len(ord_[key][0])\n",
    "\n",
    "\n",
    "# Define function to find outliers\n",
    "def find_outliers_IQR(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    IQR = q3 - q1\n",
    "    upper_bound = q3 + 1.5 * IQR\n",
    "    lower_bound = q1 - 1.5 * IQR\n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Function for handling outliers\n",
    "def func_handling_outlier(X):\n",
    "    X = X.copy()\n",
    "    X['Number of Dependents'] = X['Number of Dependents'].apply(lambda i: 3 if i > 3 else i)\n",
    "    out_var3 = ['Number of Referrals', 'Total Long Distance Charges', 'Avg Monthly GB Download', 'Total Revenue']\n",
    "    upper_bound = []\n",
    "    for i in out_var3:\n",
    "        upper_bound.append(int(find_outliers_IQR(X[i])[2]))\n",
    "    for i, j in zip(out_var3, upper_bound):\n",
    "        X.loc[X[X[i] > j].index.to_list(), i] = np.NaN\n",
    "    imputer = KNNImputer(n_neighbors=7)\n",
    "    X[out_var3] = imputer.fit_transform(X[out_var3])\n",
    "    return X\n",
    "\n",
    "# Split dataset\n",
    "X = df_transform[numeric_var + nominal_var + ordinal_var]\n",
    "y = df_transform['Churn Value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Ensure data is in DataFrame format\n",
    "X_train = pd.DataFrame(X_train, columns=numeric_var + nominal_var + ordinal_var)\n",
    "X_test = pd.DataFrame(X_test, columns=numeric_var + nominal_var + ordinal_var)\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "nominal_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "ordinal_pipeline = Pipeline(steps=[\n",
    "    ('encoder', OrdinalEncoder(categories=ord_categories))\n",
    "])\n",
    "\n",
    "# Column transformer for preprocessing\n",
    "prep_var = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numeric_var),\n",
    "        ('nominal', nominal_pipeline, nominal_var),\n",
    "        ('ordinal', ordinal_pipeline, ordinal_var)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Pipeline for handling outliers\n",
    "hand_outliers = Pipeline(steps=[\n",
    "    ('handling_outlier', FunctionTransformer(func_handling_outlier))\n",
    "])\n",
    "\n",
    "# Combined preprocessor\n",
    "combined_preprocessor = Pipeline(steps=[\n",
    "    ('hand_outliers', hand_outliers),\n",
    "    ('prep_var', prep_var)\n",
    "])\n",
    "\n",
    "# Fit the combined preprocessor first\n",
    "combined_preprocessor.fit(X_train)\n",
    "\n",
    "# Extract feature names after fitting the combined preprocessor\n",
    "nominal_feature_names = combined_preprocessor.named_steps['prep_var'].named_transformers_['nominal'].named_steps['encoder'].get_feature_names_out(nominal_var)\n",
    "feature_names = numeric_var + list(nominal_feature_names) + ordinal_var\n",
    "\n",
    "# Feature selection models\n",
    "feature_selection_models = {\n",
    "    'Random Forest': RandomForestClassifier(\n",
    "        n_estimators=200,  # Increased number of trees\n",
    "        max_depth=10,  # No limit on the depth of the trees\n",
    "        min_samples_split=10,  # Lowered to allow more splits\n",
    "        random_state=42\n",
    ")\n",
    ",\n",
    "    'XGBoost': XGBClassifier(\n",
    "        eval_metric='logloss',\n",
    "        n_estimators=200,  # Increased number of boosting rounds\n",
    "        max_depth=6,  # Keep the same depth\n",
    "        min_child_weight=5,  # Reduced to allow more splits\n",
    "        subsample=0.8,  # Keep the same subsample ratio\n",
    "        learning_rate=0.1,  # Introduce a learning rate\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv_df_fs = []\n",
    "for name, model in feature_selection_models.items():\n",
    "    feature_selector = SelectFromModel(model, threshold='mean')\n",
    "    \n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    score = cross_validate(model_pipe, X_train, y_train, cv=10, return_train_score=False, scoring=['roc_auc', 'f1', 'recall', 'precision'])\n",
    "    mean_score = []\n",
    "    for i in score.values():\n",
    "        mean_score.append(round(np.mean(i), 3))\n",
    "    cv_df_fs.append(pd.DataFrame({'Attribute': [i for i in score.keys()], f'{name}': mean_score}))\n",
    "\n",
    "score_fs = pd.DataFrame(columns=['Attribute'])\n",
    "for i in cv_df_fs:\n",
    "    score_fs = score_fs.merge(right=i, on='Attribute', how='outer')\n",
    "score_fs = score_fs.loc[2:].reset_index(drop=True)\n",
    "\n",
    "print(\"Feature Selection Model Cross-Validation Results:\")\n",
    "print(score_fs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_pipes = {}\n",
    "# Fitting the feature selection models and evaluating on the test set\n",
    "for name, model in feature_selection_models.items():\n",
    "    feature_selector = SelectFromModel(model, threshold='mean')\n",
    "    \n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    model_pipes[name] = model_pipe\n",
    "    \n",
    "    # Extract selected feature names\n",
    "    selected_mask = model_pipe.named_steps['feature_selection'].get_support()\n",
    "    selected_features = [feature for feature, selected in zip(feature_names, selected_mask) if selected]\n",
    "\n",
    "    print(f\"Selected Features for {name}: {selected_features}\")\n",
    "\n",
    "    # Transform the training and testing sets using the entire pipeline\n",
    "    X_train_transformed = model_pipe.named_steps['preprocessor'].transform(X_train)\n",
    "    X_test_transformed = model_pipe.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "    # Transform using the feature selector\n",
    "    X_train_selected = model_pipe.named_steps['feature_selection'].transform(X_train_transformed)\n",
    "    X_test_selected = model_pipe.named_steps['feature_selection'].transform(X_test_transformed)\n",
    "\n",
    "    # Create a new model pipeline with only the selected features\n",
    "    final_model_pipe = Pipeline(steps=[\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    # Fit the final model on the selected features\n",
    "    final_model_pipe.fit(X_train_selected, y_train)\n",
    "\n",
    "    # Evaluate the final model on the test set\n",
    "    y_pred = final_model_pipe.predict(X_test_selected)\n",
    "    print(f\"Classification report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Plot feature importances of the selected features\n",
    "    importances = model_pipe.named_steps['feature_selection'].estimator_.feature_importances_\n",
    "    selected_importances = [importance for importance, selected in zip(importances, selected_mask) if selected]\n",
    "    importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': selected_importances})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title(f'Feature Importance for {name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cross-validation results to DataFrame\n",
    "cv_df_long = pd.melt(score_fs, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Plot performance metrics comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Attribute\", y=\"Score\", hue=\"Model\", data=cv_df_long)\n",
    "plt.title('Model Performance Metrics Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot the learning curve with a specific scoring metric\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), scoring=None):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(scoring if scoring else \"Score\")  # Display the scoring metric used\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "# Plot learning curves for each model with scoring='recall'\n",
    "for name, model in feature_selection_models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    plot_learning_curve(model_pipe, f'Learning Curve for {name}', X_train, y_train, cv=5, scoring='recall')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all ordinal variables into one list and their respective order categories\n",
    "ordinal_var = []\n",
    "ord_categories = []\n",
    "for key in ord_:\n",
    "    ordinal_var += ord_[key][0]\n",
    "    ord_categories += [ord_[key][1]] * len(ord_[key][0])\n",
    "\n",
    "# Define function to find outliers\n",
    "def find_outliers_IQR(series):\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    IQR = q3 - q1\n",
    "    upper_bound = q3 + 1.5 * IQR\n",
    "    lower_bound = q1 - 1.5 * IQR\n",
    "    outliers = series[(series < lower_bound) | (series > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Function for handling outliers\n",
    "def func_handling_outlier(X):\n",
    "    X = X.copy()\n",
    "    X['Number of Dependents'] = X['Number of Dependents'].apply(lambda i: 3 if i > 3 else i)\n",
    "    out_var3 = ['Number of Referrals', 'Total Long Distance Charges', 'Avg Monthly GB Download', 'Total Revenue']\n",
    "    upper_bound = []\n",
    "    for i in out_var3:\n",
    "        upper_bound.append(int(find_outliers_IQR(X[i])[2]))\n",
    "    for i, j in zip(out_var3, upper_bound):\n",
    "        X.loc[X[X[i] > j].index.to_list(), i] = np.NaN\n",
    "    imputer = KNNImputer(n_neighbors=7)\n",
    "    X[out_var3] = imputer.fit_transform(X[out_var3])\n",
    "    return X\n",
    "\n",
    "# Split dataset\n",
    "X = df_transform[numeric_var + nominal_var + ordinal_var]\n",
    "y = df_transform['Churn Value']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
    "\n",
    "# Ensure data is in DataFrame format\n",
    "X_train = pd.DataFrame(X_train, columns=numeric_var + nominal_var + ordinal_var)\n",
    "X_test = pd.DataFrame(X_test, columns=numeric_var + nominal_var + ordinal_var)\n",
    "\n",
    "# Preprocessing pipelines\n",
    "num_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "nominal_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "ordinal_pipeline = Pipeline(steps=[\n",
    "\n",
    "    ('encoder', OrdinalEncoder(categories=ord_categories))\n",
    "])\n",
    "\n",
    "# Column transformer for preprocessing\n",
    "prep_var = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numeric_var),\n",
    "        ('nominal', nominal_pipeline, nominal_var),\n",
    "        ('ordinal', ordinal_pipeline, ordinal_var)\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Pipeline for handling outliers\n",
    "hand_outliers = Pipeline(steps=[\n",
    "    ('handling_outlier', FunctionTransformer(func_handling_outlier))\n",
    "])\n",
    "\n",
    "# Combined preprocessor\n",
    "combined_preprocessor = Pipeline(steps=[\n",
    "    ('hand_outliers', hand_outliers),\n",
    "    ('prep_var', prep_var)\n",
    "])\n",
    "\n",
    "# Fit the combined preprocessor first\n",
    "combined_preprocessor.fit(X_train)\n",
    "\n",
    "# Extract feature names after fitting the combined preprocessor\n",
    "nominal_feature_names = combined_preprocessor.named_steps['prep_var'].named_transformers_['nominal'].named_steps['encoder'].get_feature_names_out(nominal_var)\n",
    "feature_names = numeric_var + list(nominal_feature_names) + ordinal_var\n",
    "\n",
    "# Feature selection with Logistic Regression using RFE\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "feature_selector = RFE(model, n_features_to_select=10)  # Select top 10 features\n",
    "\n",
    "model_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', combined_preprocessor),\n",
    "    ('feature_selection', feature_selector),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "cv_df_fs_LR = []\n",
    "\n",
    "# Perform cross-validation\n",
    "score = cross_validate(model_pipe, X_train, y_train, cv=10, return_train_score=False, scoring=['roc_auc', 'f1', 'recall', 'precision'])\n",
    "\n",
    "# Convert cross-validation results to DataFrame\n",
    "cv_df_fs_LR = pd.DataFrame({\n",
    "    'Attribute': [i for i in score.keys()],\n",
    "    'Logistic Regression': [round(np.mean(score[i]), 3) for i in score.keys()]\n",
    "})\n",
    "\n",
    "print(\"Feature Selection Model Cross-Validation Results:\")\n",
    "print(cv_df_fs_LR)\n",
    "\n",
    "# Directly assign to score_fs_LR\n",
    "score_fs_LR = cv_df_fs_LR.loc[2:].reset_index(drop=True)\n",
    "\n",
    "# Fitting the feature selection model and evaluating on the test set\n",
    "model_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Extract selected feature names\n",
    "selected_mask = model_pipe.named_steps['feature_selection'].get_support()\n",
    "selected_features = [feature for feature, selected in zip(feature_names, selected_mask) if selected]\n",
    "\n",
    "print(f\"Selected Features for Logistic Regression: {selected_features}\")\n",
    "\n",
    "# Transform the training and testing sets using the entire pipeline\n",
    "X_train_transformed = model_pipe.named_steps['preprocessor'].transform(X_train)\n",
    "X_test_transformed = model_pipe.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Transform using the feature selector\n",
    "X_train_selected = model_pipe.named_steps['feature_selection'].transform(X_train_transformed)\n",
    "X_test_selected = model_pipe.named_steps['feature_selection'].transform(X_test_transformed)\n",
    "\n",
    "# Create a new model pipeline with only the selected features\n",
    "final_model_pipe = Pipeline(steps=[\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Fit the final model on the selected features\n",
    "final_model_pipe.fit(X_train_selected, y_train)\n",
    "\n",
    "# Evaluate the final model on the test set\n",
    "y_pred = final_model_pipe.predict(X_test_selected)\n",
    "print(f\"Classification report for Logistic Regression:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot learning curves for different metrics\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "\n",
    "for metric in metrics:\n",
    "    plot_learning_curve(model_pipe, f'Learning Curve for Logistic Regression ({metric})', X_train, y_train, cv=5, scoring=metric)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the cross-validation results to long format\n",
    "cv_df_long_LR = pd.melt(cv_df_fs_LR, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Define the desired order of metrics\n",
    "desired_order = ['test_f1', 'test_precision', 'test_recall', 'test_roc_auc']\n",
    "\n",
    "# Convert the 'Attribute' column to a categorical type with the specified order\n",
    "cv_df_long_LR['Attribute'] = pd.Categorical(cv_df_long_LR['Attribute'], categories=desired_order, ordered=True)\n",
    "\n",
    "# Plot performance metrics comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Attribute\", y=\"Score\", hue=\"Model\", data=cv_df_long_LR)\n",
    "plt.title('Logistic Regression Performance Metrics Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined Models metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Logistic Regression cross-validation results to long format\n",
    "cv_df_long_LR = pd.melt(cv_df_fs_LR, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Convert other models' cross-validation results to long format\n",
    "cv_df_long_other = pd.melt(score_fs, id_vars=[\"Attribute\"], var_name=\"Model\", value_name=\"Score\")\n",
    "\n",
    "# Ensure there are no duplicate entries\n",
    "cv_df_long_LR = cv_df_long_LR.drop_duplicates(subset=['Attribute', 'Model'])\n",
    "cv_df_long_other = cv_df_long_other.drop_duplicates(subset=['Attribute', 'Model'])\n",
    "\n",
    "# Combine the two DataFrames\n",
    "combined_df_long = pd.concat([cv_df_long_LR, cv_df_long_other], axis=0)\n",
    "\n",
    "# Reset index to ensure no duplicate indices\n",
    "combined_df_long = combined_df_long.reset_index(drop=True)\n",
    "\n",
    "# Define the desired order of metrics\n",
    "desired_order = ['test_f1', 'test_precision', 'test_recall', 'test_roc_auc']\n",
    "\n",
    "# Convert the 'Attribute' column to a categorical type with the specified order\n",
    "combined_df_long['Attribute'] = pd.Categorical(combined_df_long['Attribute'], categories=desired_order, ordered=True)\n",
    "\n",
    "# Plot performance metrics comparison for all models\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=\"Attribute\", y=\"Score\", hue=\"Model\", data=combined_df_long)\n",
    "plt.title('Model Performance Metrics Comparison')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Metric')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC AUC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for name, model in feature_selection_models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve for Logistic Regression\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define the Logistic Regression pipeline\n",
    "model_pipe = Pipeline(steps=[\n",
    "    ('preprocessor', combined_preprocessor),\n",
    "    ('feature_selection', feature_selector),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "model_pipe.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot(fpr, tpr, lw=2, label=f'Logistic Regression (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot the diagonal line representing random guessing\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined ROC AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot ROC curves for other models in feature_selection_models\n",
    "for name, model in feature_selection_models.items():\n",
    "    model_pipe = Pipeline(steps=[\n",
    "        ('preprocessor', combined_preprocessor),\n",
    "        ('feature_selection', feature_selector),\n",
    "        ('model', model)])\n",
    "    model_pipe.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model_pipe.predict_proba(X_test)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "# Plot ROC curve for Logistic Regression\n",
    "# Define the Logistic Regression pipeline\n",
    "model_pipe_lr = Pipeline(steps=[\n",
    "    ('preprocessor', combined_preprocessor),\n",
    "    ('feature_selection', feature_selector),\n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "model_pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities for the test set\n",
    "y_pred_proba_lr = model_pipe_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC curve and AUC for Logistic Regression\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
    "roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "\n",
    "# Plot the ROC curve for Logistic Regression\n",
    "plt.plot(fpr_lr, tpr_lr, lw=2, label=f'Logistic Regression (AUC = {roc_auc_lr:.2f})', color='red')  # Use a distinct color\n",
    "\n",
    "# Plot the diagonal line representing random guessing\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "coefficients = final_model_pipe.named_steps['model'].coef_[0]\n",
    "importance_df = pd.DataFrame({'Feature': selected_features, 'Coefficient': coefficients})\n",
    "importance_df = importance_df.sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Coefficient', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importance for Logistic Regression')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
